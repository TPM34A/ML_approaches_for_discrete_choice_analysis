{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Choice Analysis: micro-econometrics and machine learning approaches\n",
    "\n",
    "## `Lab exercise 01`<br> `Artificial Neural Networks`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q2 2022**<br>\n",
    "**Instructor:** Sander van Cranenburgh <br>\n",
    "**TAs:**  Francisco Garrido Valenzuela & Lucas Spierenburg <br>\n",
    "\n",
    "## `Workspace set-up`\n",
    "**Option 1: Google Colab**<br>\n",
    "    Uncomment the following cell if you are running this notebook on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/TPM34A/ML_approaches_for_discrete_choice_analysis\n",
    "#!pip install -r ML_approaches_for_discrete_choice_analysis/requirements.txt\n",
    "#!mv \"/content/ML_approaches_for_discrete_choice_analysis/Lab sessions/data\" /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Local environment**<br>\n",
    "    Uncomment the following cell if you are running this notebook on your local environment, to install all dependencies on your Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Application: Swiss mode choice` <br>\n",
    "In this lab session we will use ANN to predict mode choice behaviour. Being able to predict model choice behaviour is highly policy relevant. Policy makers use it to develop policies aimed at reducing externalities associated with travel behaviour, such as e.g. fuel levies, subsidization of public transport, speed limits. Additionally, they use it for planning transport infrastructure. Based on travel demand forecast e.g. roads are widened or new train connections are built. Traditionally, mode choice predictions are based on classic econometric tools. Recently, the high prediction performance of ML methods appeal researchs to use ML for model choice predictions too. In this lab session you will develop ANNs to predict mode behaviour and assess its performance<br>\n",
    "\n",
    "\n",
    "**Learning objectives**. After completing the following exercises you will be able to: <br>\n",
    "1. Prepare (choice) data for training Artificial Neural Networks\n",
    "2. Train MultiLayerPerceptron (MLP) - a praticular type of neural network - for a classification task<br>\n",
    "3. Tune the hyperparameter and network architectures to improve the model performance<br>\n",
    "4. Assess the performance of of competing models, based on various performance measures, including confusion matrices, and Precision, Recall, F1 scores and Matthew'ss coefficient<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Organisation`\n",
    "This lab session comprises **`6`** parts:\n",
    "1. Preparing (choice) data for training Artificial Neural Networks\n",
    "2. Training a MultiLayerPerceptron (MLP) neural network\n",
    "3. Using Early stopping to avoid overfitting\n",
    "4. Using k-fold cross validation to evaluate generalisation performance\n",
    "5. Tuning hyperparameter\n",
    "6. Evaluating performance of trained models\n",
    "\n",
    "and comprises **`7`** exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python packages and modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "\n",
    "# Import selected functions and classes from Python packages\n",
    "from os import getcwd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_validate\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, log_loss, matthews_corrcoef, make_scorer, classification_report\n",
    "\n",
    "# Setting\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Preparing (choice) data for training Artificial Neural Networks**\n",
    "To prepare the data set, we will:<br>\n",
    "    1.1 **Load** the data set<br>\n",
    "    1.2 **Inspect** and **Clean** the data set<br>\n",
    "    1.3 **Discover and visualise** the data <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.1 Load the choice data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "datafile_path = os.path.join(getcwd(),'data','')\n",
    "print(datafile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mode choice data into a pandas DataFrame\n",
    "df = pd.read_csv(datafile_path + 'swissmetro.dat',sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.2 Inspect and clean the data**<br>\n",
    "Before starting to analyse your data, make sure you understand what features are in your data.<br>\n",
    "Therefore, it is highly recommended to look at the description of the data set. [Click here](https://github.com/TPM34A/Admin_2022/blob/main/lab%20sessions/lab_ex02/data/CS_SwissmetroDescription.pdf) to open the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the range and distribution statistics \n",
    "df.describe()\n",
    "\n",
    "# Inspect the data types in the df\n",
    "# df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "# Only keep data for purposes 'Commute\" and \"Business\"\n",
    "df.drop(df[(df.PURPOSE != 1) & (df.PURPOSE != 3)].index, inplace=True) \n",
    "\n",
    "# Drop rows with unknown choices (CHOICE == 0)\n",
    "df.drop(df[df.CHOICE == 0].index, inplace=True) \n",
    "\n",
    "# In case of missing values, we replace them with 0\n",
    "df.fillna(0, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1.3 Discovering and visualising the data**<br>\n",
    "Before starting to analyse your data with models, it is advisable to start with some **descriptive analyses**.<br>\n",
    "Therefore is recommended to look first at the distribution and correlations of key feature in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 1: Is the data set (im)balanced?``\n",
    "`A` Create a histogram showing how often TRAIN, SM and CAR are chosen. Do not forget to add labels to the columns<br>\n",
    "`B` Interpret the the histogram. Is the data set imbalanced? <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A) We Create a histogram using sns, where we count the target variable \"CHOICE\"\n",
    "# B) We see that the target variable is not well balanced. SM is much more often chosen than TRAIN or CAR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 1, figsize=(8, 4))\n",
    "axes = sns.countplot(x = \"CHOICE\", data = df)\n",
    "ylabels = ['Train', 'SM', 'Car']\n",
    "axes.set_xticklabels(ylabels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 2: Explore correlations between features``\n",
    "`A` Create a heatmap to identify what features particulalrly correlate with the **CHOICE**<br>\n",
    "`B` Identify the features that strongly correlate (corr >0.9) (if any). Do they make sense? <br>\n",
    "Hint: to do so you will need the description of the features [Click here](https://github.com/TPM34A/Admin_2022/blob/main/lab%20sessions/lab_ex02/data/CS_SwissmetroDescription.pdf) to open the pdf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A) Because CHOICE is a categorical variable, we first must create 3 new hot-encoded variables for CHOCICE ==1, CHOICE == 2, and CHOICE == 3.\n",
    "# After that, we compute the correlations, and visualise the correlation in a heatmap.\n",
    "# B) In general we seen that travel cost and travel times strongly correlate, within and across modes. This makes perfect sense.\n",
    "# GA negatively correlates with SM_CO and TRAIN_CO. This is by design, as we set costs to zero for concession card holders\n",
    "# TICKET correlates negatively with TRAIN and SM costs. TICKET is a categorical variable, with various forms of discount. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# heatmap of correlations\n",
    "# Create plot\n",
    "f, axes = plt.subplots(figsize=(16,16))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "# Since choice is categorical, we one-hot encode them (a.k.a dummy encoding)\n",
    "dff = df.copy()\n",
    "dff['CHOICE1'] = dff['CHOICE'] == 1\n",
    "dff['CHOICE2'] = dff['CHOICE'] == 2\n",
    "dff['CHOICE3'] = dff['CHOICE'] == 3\n",
    "\n",
    "# Compute correlation matrix\n",
    "corr = dff.corr()\n",
    "\n",
    "# Create upper triangular matrix to mask the upper triangular part of the heatmap\n",
    "corr_mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "\n",
    "# Generate a custom diverging colormap (because it looks better)\n",
    "corr_cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Plot the heatmap\n",
    "sns.heatmap(corr, mask = corr_mask, cmap=corr_cmap, annot=True,square = True, linewidths=.5, ax = axes)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Training a MultiLayerPerceptron (MLP) neural network**\n",
    "To train Artifical Neural Networks we take the following steps:<br>\n",
    "    2.1 **Scaling** the features<br>\n",
    "    2.2 **Splitting** the data in a training and test data set<br>\n",
    "    2.3 **Creating MLP** object<br>\n",
    "    2.4 **Training the MLP** on the train data<br>\n",
    "    2.5 **Evaluating** the performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Scaling the features**<br>\n",
    "To efficiently train ANNs it strongly recommended to **scale** (a.k.a. normalise) the features. There are serval ways to scale your data. A commonly used scaler of `sk-learn` is called 'StandardScaler'. This scaler normalises the variance and shift the location of the distribution to zero, see https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates the list of features that need to be scaled\n",
    "# Importantly, this excludes the availabilities and the choice\n",
    "features2scale = ['PURPOSE', 'FIRST', 'TICKET','WHO', 'LUGGAGE', 'AGE','MALE', 'INCOME', 'GA','ORIGIN', 'DEST',\n",
    "                 'TRAIN_TT', 'TRAIN_CO', 'TRAIN_HE',\n",
    "                 'SM_TT', 'SM_CO','SM_HE', 'SM_SEATS',\n",
    "                 'CAR_TT', 'CAR_CO']\n",
    "\n",
    "# Initiate scaler object & fit to data\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(df.loc[:,features2scale])  \n",
    "\n",
    "# Create new dataframe X_scaled containg the scaled features AND the (unscaled) ones: availabilities, Group, Survey, Choice\n",
    "X_scaled = df.copy()\n",
    "X_scaled.at[:, features2scale] = scaler.transform(df.loc[:,features2scale])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Splitting the data in a train set and a test set**<br>\n",
    "Training ML models always involves a **train** and a **test** data set. The train set is used to update the weights of the model. Tha test set is used to evaluate the **generalisation performance** of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the list of features that we want to use in the model\n",
    "features = ['PURPOSE', 'FIRST', \n",
    "            'TICKET', 'WHO', 'LUGGAGE', 'AGE', \n",
    "            'MALE', 'INCOME', 'GA', 'ORIGIN', \n",
    "            'DEST', 'TRAIN_TT', 'TRAIN_CO', \n",
    "            'TRAIN_HE', 'SM_TT', 'SM_CO', \n",
    "            'SM_HE', 'SM_SEATS', 'CAR_TT', 'CAR_CO',\n",
    "            'TRAIN_AV', 'SM_AV', 'CAR_AV']\n",
    "\n",
    "# Create the target\n",
    "Y = df['CHOICE']\n",
    "\n",
    "# Split the data using sk-learn's `train_test_split` function\n",
    "# Note that we use 60% for training and 40% for testing\n",
    "# Note that we set the random_state, in order to replicate results later (do not change) \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_scaled[features], Y, random_state = 12345, test_size = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.3 Creating the MLP object**<br>\n",
    "A MultiLayerPerceptron (MLP) is a fully-connected feed-forward neural network. We create the MLP using `sk-learn's` MLPClassifier function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 'plain vanilla' MLP object\n",
    "# Declare the number of layers and nodes per layer\n",
    "# layers = (a,b) means two layer with a nodes in the 1st hidden layer and b nodes in the 2nd hidden layer\n",
    "layers = (10)\n",
    "\n",
    "# Define MLP architecture, optimiser and hyperparameters:\n",
    "# We use Adam optimiser\n",
    "# We use the learning rate to 0.001\n",
    "# We use L2 regularisation of 0.1\n",
    "# We use a batch size of 250 observations\n",
    "# We use tanhg activation (transfer function)\n",
    "# We set the max number of epochs to 2000\n",
    "mlp = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = 0.001, alpha=0.1, batch_size=250, activation = 'tanh', max_iter = 2000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.4 Training the MLP**<br>\n",
    "To train the MLP we use the '.fit' function.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the MLP using the train data.\n",
    "mlp.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is good practice to look at the learning curve, to see how much the model has improved from the initial point,\n",
    "# and how is has evolved over epochs.\n",
    "fig, ax = plt.subplots(figsize = (16,8))\n",
    "plt.plot(mlp.loss_curve_)\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cross entropy loss')\n",
    "ax.grid(True,linewidth = 0.5)\n",
    "ax.set_ylim(0.0,1.2)\n",
    "ax.set_xlim(0,mlp.n_iter_)\n",
    "ax.set_title(f'Cross entropy loss on the TRAINING DATA. \\nBest CE = {mlp.loss_:4.3f}')\n",
    "# The plot illustrates that the training stops when the training loss does no longer improve more that a given tolerance (e.g. 1e-6), or reaches max_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.5 Evaluating the model performance** <br>\n",
    "Machine learning researchers usually evaluate the performance of a classification model using the cross-entropy. A lower cross entrop indicates a better model. In contrast, choice modellers usually look at the Log-likelihood (LL) and the rho-square. <br> A high LL and rho-square indicate a better model. <br>\n",
    "As there is no standard function that outputs the evaluation metrics of both disciplines, below we create our **own evaluation function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an eavluation function that returns key evaluation metrics: LL, LL0, cross_entropy, rho_sq\n",
    "# To compute these performance metrics, the function takes as inputs:\n",
    "#   - the predicted probabilities (prob)\n",
    "#   - the choices (Y)\n",
    "#   - the availabilities (AV)\n",
    "def eval_performance(prob,Y,AV):\n",
    "    \n",
    "    # Calculate the likelihood of the data given the model\n",
    "    LL = np.sum(np.log(np.sum(prob*Y,axis=1)))\n",
    "\n",
    "    # Calculate the Null-loglikelihood\n",
    "    LL0 = np.sum(np.log(np.divide(1,np.sum(AV,axis=1))))\n",
    "\n",
    "    # Calculate cross-entropy\n",
    "    cross_entropy =  -LL/len(AV)\n",
    "    \n",
    "    # Calculate the rho_sq\n",
    "    rho_sq = 1 -(LL/LL0)\n",
    "    return LL, LL0, cross_entropy, rho_sq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our function to evaluate and compare the performance of our MLP on the training and test data sets\n",
    "eval_train = eval_performance(mlp.predict_proba(X_train),np.transpose([Y_train ==1,Y_train ==2,Y_train ==3]), X_train[['TRAIN_AV','SM_AV','CAR_AV']])\n",
    "eval_test  = eval_performance(mlp.predict_proba(X_test), np.transpose([Y_test  ==1,Y_test  ==2,Y_test  ==3]), X_test[['TRAIN_AV','SM_AV','CAR_AV']])\n",
    "\n",
    "# Print the results\n",
    "print('Model performance of the plain vanilla MLP:')\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "results = pd.DataFrame({'data set':     ['Train','Test'],\n",
    "                        'LL':           [eval_train[0], eval_test[0]],\n",
    "                        'LL0':          [eval_train[1], eval_test[1]],\n",
    "                        'cross_entropy':[eval_train[2], eval_test[2]],\n",
    "                        'rho_sq':       [eval_train[3], eval_test[3]]})\n",
    "print(results.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get a sense of how well this plain vanilla MLP performs, below are the same performance metrics for a RUM-MNL model printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nModel performance of plain vanilla linear-additive RUM-MNL model:')\n",
    "print('LL\\t\\t -5331.252 \\nLL0\\t\\t -6964.663\\ncross_entropy\\t 0.79 \\nrho_sq\\t\\t 0.23')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 3: Does using more nodes improve the model performance?``\n",
    "`A` Calculate the number of weights consumed by the current MLP with 10 hidden nodes in 1 hidden layer. <br>\n",
    "`B` Retrain your model several times with {10,30,60,90} nodes. Report the cross-entropy performance on the train and test data sets.<br>\n",
    "`C` Does increasing the number of nodes lead to a lower cross-entropy on the train and or test set? What is happening?<br>\n",
    "`D` Suppose you would have many more choice observations from this survey. Would that enable you to develop a much better model with a cross entropy performance of say <0.10 (on the test set)? Explain yor answer. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A Weight = (23 * 10 + 10 * 3)  + Bias = 10 + 3 --> 273\n",
    "# B 10 nodes    0.56    |   0.64\n",
    "#   30 nodes    0.41    |   0.67\n",
    "#   60 nodes    0.29    |   0.68\n",
    "#   90 nodes    0.23    |   0.69\n",
    "# C On the training set the cross entropy goes down. On the test set is does not. Actually the performance gets worse\n",
    "# D No probably not. There is irreducible error in choice data. Human choice behaviour is to some extent random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Using Early stopping to avoid overfitting**\n",
    "Early stopping refers to a technique that stops the training of the network when the performance on the test data set no longer improves. Thereby, early stopping avoids the model to overfit the data. It essetially stops the training before the model can overfit.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note this is the same 'plain vanilla' MLP, but now we set early_stopping = True\n",
    "# A validation_fraction is added. This fraction is the proportion of training data to set aside as validation set for early stopping. This data set is used to determine when to stop. \n",
    "# The training stops when the performance on the validation data set does not improve for n_iter_no_change in a row\n",
    "# We use the MLP with 10 nodes and one hidden layer again.\n",
    "layers = (10)\n",
    "n_iter_no_change = 10\n",
    "mlp_early_st = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = 0.001, alpha=0.1, batch_size=250, activation = 'tanh', max_iter = 2000, early_stopping=True, n_iter_no_change = n_iter_no_change,validation_fraction = 0.25) \n",
    "\n",
    "# Train the MLP using the train data\n",
    "# Note that we use df.values here. This is due to a small bug in sk-learn. Without .values sk-learn still works, but prompts some warnings\n",
    "mlp_early_st.fit(X_train.values, Y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is good practice to look at the learning curve, to see how much the model has improved from the initial point,\n",
    "# and how is has evolved over epochs.\n",
    "# The learning curve plot also illustrates well what early stopping does.\n",
    "fig, ax = plt.subplots(figsize = (16,8))\n",
    "plt.plot(mlp.loss_curve_,label=f'MLP with {mlp.hidden_layer_sizes} hidden nodes, WITHOUT early stopping')\n",
    "plt.plot(mlp_early_st.loss_curve_,label=f'MLP with {mlp_early_st.hidden_layer_sizes} hidden nodes, WITH early stopping')\n",
    "ax.set_xlabel('Epoch')\n",
    "ax.set_ylabel('Cross entropy loss')\n",
    "ax.grid(True,linewidth = 0.5)\n",
    "ax.set_ylim(0.0,1.2)\n",
    "ax.set_xlim(0,mlp.n_iter_)\n",
    "ax.set_title('Effect of early stopping')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use our evaluation function again to evaluate performance of the MLP with early stopping on the training and test data set\n",
    "eval_train_early_st = eval_performance(mlp_early_st.predict_proba(X_train.values),np.transpose([Y_train ==1,Y_train ==2,Y_train ==3]), X_train[['TRAIN_AV','SM_AV','CAR_AV']])\n",
    "eval_test_early_st  = eval_performance(mlp_early_st.predict_proba(X_test.values), np.transpose([Y_test  ==1,Y_test  ==2,Y_test  ==3]), X_test[['TRAIN_AV','SM_AV','CAR_AV']])\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "results_early_st = pd.DataFrame({'data set':     ['Train','Test'],\n",
    "                        'LL':           [eval_train_early_st[0], eval_test_early_st[0]],\n",
    "                        'LL0':          [eval_train_early_st[1], eval_test_early_st[1]],\n",
    "                        'cross_entropy':[eval_train_early_st[2], eval_test_early_st[2]],\n",
    "                        'rho_sq':       [eval_train_early_st[3], eval_test_early_st[3]]})\n",
    "print(results_early_st.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 4: Can we overfit with early stopping?``\n",
    "`A` Did early stopping reduced overfitting? How can you see this from the printed results?<br>\n",
    "`B` Try if early stoppping also helps to avoid overfitting when using more nodes, e.g. 90 nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A Yes, it works very well in voiding overfitting. This can be seen by the comparively small gap between the cross entropy on the train and test data sets\n",
    "# B Yes, even with 90 nodes, the gap in the cross entropy between the train and the test data sets stays small"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Using k-fold cross validation to evaluate generalisation performance**\n",
    "k-fold cross validation is commonly used to more accurately **evaluate the generalisation performance** of a given network. It improves a simple train-test split approach in that it systematically cuts the data set in k pieces. k-fold cross validation is especially crucial when tuning hyperparameters (as we will see later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP object (plain vanilla MLP without early stopping)\n",
    "layers = 10\n",
    "mlp_cv = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = 0.001, alpha=0.1, batch_size=250, activation = 'tanh', max_iter = 2000) \n",
    "\n",
    "# Create scoring function\n",
    "# It is necessary to create a scoring function when working with cross_validate of sk-learn\n",
    "# We set `greater_is_better` to `False` as we are minimising cross entropy loss\n",
    "logloss = make_scorer(log_loss, greater_is_better = False, needs_proba = True)\n",
    "\n",
    "# Apply cross_validate, using e.g. 5 folds\n",
    "# Since we use cross-validation training takes n_folds times longer than using a train-test split\n",
    "n_folds = 5\n",
    "cv_results = cross_validate(mlp_cv,X_scaled[features],Y,cv = n_folds, scoring=logloss,return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the train and test performance in a bar plot, for each fold\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x_axis = np.arange(n_folds)\n",
    "ax.bar(x_axis + -0.125, -cv_results['train_score'], color = 'b', width = 0.25,label = 'Train data set')\n",
    "ax.bar(x_axis +  0.125, -cv_results['test_score'], color = 'g', width = 0.25,label = 'Test data set')\n",
    "ax.set_xlabel('fold #')\n",
    "ax.set_ylabel('cross entropy')\n",
    "ax.grid()\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `This cross validation analysis supports the finding that a plain vanilla MLP (i.e. without early stopping) overfits the data considerably`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Tuning hyperparameter**\n",
    "When training MLPs (and most other ML models) there are several parameters we can **'tune'** (optimise) to improve the model's performance. The process of doing this is called **hyperparameter tuning**. Hyperparameter tuning can be done manually, but that is unwieldy. The GridSearchCV function in `sk-learn` automates the hyperparemeter tuning process. When tuning the hyperparameters, it is mandatory to use a k-fold cross validation approach. Otherwise, there is a risk of overfitting on the test set *because* the parameters can be tweaked until the estimator performs optimally on the test data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create MLP object (plain vanilla MLP)\n",
    "mlp_gs = MLPClassifier(activation = 'tanh', solver='adam', batch_size=250, max_iter=3000)\n",
    "\n",
    "# Define the hyperparameter search space\n",
    "# 'hidden_layer_sizes' defines the number of nodes and layers\n",
    "# 'alpha' governs the L2 regularisation\n",
    "# 'learning_rate_init' governs the learning rate.\n",
    "hyperparameter_space = {\n",
    "    'hidden_layer_sizes': [(10),(10,10),(30,30)],\n",
    "    'alpha': [1,1e-2,1e-4],\n",
    "    'learning_rate_init': [0.01,0.001,0.0001]}\n",
    "\n",
    "# Create scoring function\n",
    "logloss = make_scorer(log_loss, greater_is_better = False, needs_proba = True)\n",
    "\n",
    "# Create the grid_search object, with using the MLP classifier\n",
    "folds = 5 # Number of cross validation splits\n",
    "mlp_gridsearch = GridSearchCV(mlp_gs, hyperparameter_space, n_jobs=-1, cv=folds,scoring = logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute the training/gridsearch\n",
    "# Note that this is computationally expensive! \n",
    "# It may take up to 5 minutes, since 3 x 3 x 3 = 27 models need to be trained, each with 5 folds (=135)\n",
    "mlp_gridsearch.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save your hyperparameter tuning results, so we only have to do it once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save your model \n",
    "filename = 'my_tuned_model.sav'\n",
    "pickle.dump(mlp_gridsearch, open(filename,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your model (if you have a saved model)\n",
    "# mlp_gridsearch = pickle.load(open(filename,'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualise the hyperparameter tuning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect results into a new pandas dataframe\n",
    "df_gridsearch = pd.DataFrame.from_dict(mlp_gridsearch.cv_results_)\n",
    "    \n",
    "# Add new column with a label for the hyperparameter combinations %% NOT SURE IF THIS IS BET WAY TO DO THIS IN PYTHON   \n",
    "df_gridsearch['gs_combinations'] = 'L2 = '+ df_gridsearch['param_alpha'].astype('str') + '; Learning_rate = '+ df_gridsearch['param_learning_rate_init'].astype('str') + '; Layers = ' + df_gridsearch['param_hidden_layer_sizes'].astype('str')\n",
    "df_gridsearch = df_gridsearch.sort_values('rank_test_score')\n",
    "\n",
    "# Visualise deviation in performance across hyper parameter settings\n",
    "plt.figure(figsize = (16,6))\n",
    "ax = sns.barplot(x = df_gridsearch.gs_combinations,y=-df_gridsearch.mean_test_score,palette=\"Blues_d\",)\n",
    "plt.xticks(rotation=90)\n",
    "\n",
    "print('Best hyperparameters found:\\t', mlp_gridsearch.best_params_)\n",
    "print('Best model performance:\\t\\t', -mlp_gridsearch.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 5: Hyperparameter tuning``\n",
    "`A` What hyperparameter turns out to be particularly impactful on the model performance?<br>\n",
    "`B` Can you think of reasons why this could be the case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A The L2 regurlarisation turns out to be very important. The top X best performing models all have alpha = 1\n",
    "# B The data are comparitively small, relatively to the number of weights of the network. \n",
    "# Therefore, we not to strongly penalise the weights in the model to avoid it from capturing quirks in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**(Re)Training the model with optimised hyperparameters**<br> \n",
    "After completing hypertuning, you know the optimal hyperparameters. <br>\n",
    "Therefore, after hypertuning we always retrain the model using the optimised hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new mlp object using the optimised hyperparameters, just using the train/test split\n",
    "layers = mlp_gridsearch.best_params_['hidden_layer_sizes']\n",
    "lr = mlp_gridsearch.best_params_['learning_rate_init']\n",
    "alpha = mlp_gridsearch.best_params_['alpha']\n",
    "mlp_gs = MLPClassifier(hidden_layer_sizes = layers, solver='adam', learning_rate_init = lr, alpha=alpha, batch_size=250, activation = 'tanh', max_iter = 2000) \n",
    "\n",
    "# Train the model\n",
    "mlp_gs.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also evaluate performance on the hypertuned model using our evaluation function\n",
    "# Note we use the full data here\n",
    "eval_gridsearch = eval_performance(mlp_gs.predict_proba(X_scaled[features]),np.transpose([Y ==1,Y ==2,Y ==3]), X_scaled[['TRAIN_AV','SM_AV','CAR_AV']])\n",
    "\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "results_clf = pd.DataFrame({'data set': ['All data'],\n",
    "                        'LL':           [eval_test[0]],\n",
    "                        'LL0':          [eval_test[1]],\n",
    "                        'cross_entropy':[eval_test[2]],\n",
    "                        'rho_sq':       [eval_test[3]]})\n",
    "print(results_clf.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **6. Evaluating and comparing performances across trained models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the performance of a wide range is metrics are available, beyond generalisation performance.<br>\n",
    "Here, we look at:<br>\n",
    "i. Confusion matrix<br>\n",
    "ii. Precision, Recall, and F1-score<br>\n",
    "iii. Matthews correlation coefficient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i) Confusion matrix**<br>\n",
    "Confusion matrices shows counts from predicted and actual outcomes. The counts on the diagonal are correctly classified outcomes (the model predictions and the ground true are the same). The counts on the off diagonal elements are the misclassified outcomes. Hence, the best classifier will have a confusion matrix with only diagonal elements and the rest of the elements set to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the choices for the test data set, using the MLP trained with early stopping and MLP with hyperparameters tuned\n",
    "Y_pred_early_st  = mlp_early_st.predict(X_test.values)  # 0/1 predictions of MLP trained with early stopping\n",
    "Y_pred_gs = mlp_gs.predict(X_test)                      # 0/1 predictions of MLP with hyperparameters tuned\n",
    "\n",
    "# Show the confusion matrices, to compare the hyperparameter tuned network with the early stopping network\n",
    "fig, ax = plt.subplots(2,2,figsize = (16,12))\n",
    "fig.set_tight_layout(True)\n",
    "cm1 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_early_st, display_labels = ylabels, normalize=None,  ax=ax[(0,0)])\n",
    "cm2 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_early_st, display_labels = ylabels, normalize='true',ax=ax[(1,0)])\n",
    "cm3 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_gs, display_labels = ylabels, normalize=None,  ax=ax[(0,1)])\n",
    "cm4 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_gs, display_labels = ylabels, normalize='true',ax=ax[(1,1)])\n",
    "\n",
    "# Add titles\n",
    "cm1.ax_.set_title(f'MLP with {mlp_early_st.hidden_layer_sizes} nodes \\n trained with early stopping')\n",
    "cm2.ax_.set_title(f'MLP with {mlp_early_st.hidden_layer_sizes} nodes \\n trained with early stopping')\n",
    "cm3.ax_.set_title(f'MLP with {mlp_gs.hidden_layer_sizes} nodes \\n hyperparameters tuned')\n",
    "cm4.ax_.set_title(f'MLP with {mlp_gs.hidden_layer_sizes} nodes \\n hyperparameters tuned')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 6: Model accuracy``\n",
    "Accuracy is defined as the true positives over the total number of cases.<br>\n",
    "`A` Manually compute the prediction accuracy of the model with early stopping and the model with hyperparameter tuning<br>\n",
    "`B` For which class (Train/SM/Car) does the hyperparameter tuning improves the prediction accuracy most?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A Accuracy = diagonal element / all\n",
    "#   with early stopping: (122+1421+197)/2708 = 0.64\n",
    "#   with hypertuning:    (177+1386+395)/2708 = 0.72\n",
    "# B Delta per class \n",
    "# TRAIN: 0.34 --> 0.49 = 0.15\n",
    "# SM:    0.87 --> 0.85 = -0.02\n",
    "# CAR:   0.28 --> 0.56 = 0.28\n",
    "# Prediction accuracy improves most for class CAR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii) Precision, Recall, and F1**<br>\n",
    "Looking at the confusion matrices, the improvements in prediction accuracy due to the hyperparameter tuning may not seem very impressive. However, one should keep in mind that 0/1 predictions are sensitive to class imbalances, which are present in these data. Moreover, accuracy can be a misleading metric for imbalanced data sets. A naive model that would simply always predict \"SM\" will already do quite good."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the model performance in more depth, we thus must look at the predictions at the level of the classes.<br> \n",
    "Next, we compute Precision, Recall, and F1 score.<br>\n",
    "* **Precision** Tells you what fraction of predictions for a given class are actually of that class.<br>\n",
    "* **Recall** Tells what fraction of all observations belonging to a given class are correctly predicted as such by the model. Recall is also known as True Positive Rate (TPR), Sensitivity, Probability of Detection. <br>\n",
    "* **F1 score** combines precision and recall into a single measure. Mathematically it’s the harmonic mean of precision and recall. It can be calculated as follows: <br>\n",
    "%% ADD F1 score formula here %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the precision, recal and f1 score we conveniently use sk-learn's 'classification_report' functionality\n",
    "print('Classification report for plain vanilla MLP with early stopping\\n',classification_report(Y_test,Y_pred_early_st, target_names= ylabels))\n",
    "print('\\nClassification report for plain vanilla MLP with hyperparameters tuned\\n',classification_report(Y_test,Y_pred_gs, target_names= ylabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii) Matthew's correlation coefficient**<br>\n",
    "Another commonly used metric to evaluate the prediction performance while accounting for imbalances in the data set is  matthews correlation coefficient. Matthews Correlation Coefficient (MCC) is generally regarded as being one of the best measures to describe the confusion matrix of true and false positives and negatives by a **single number**, even if the classes are of very different sizes. The MCC is in essence a correlation coefficient value between -1 and +1. A coefficient of +1 represents a perfect prediction, 0 an average random prediction and -1 an inverse prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taking into account the imbalances of the data\n",
    "print(f'Matthews correlation coefficient for plain Vanilla MLP with early stopping:\\t {matthews_corrcoef(Y_test, Y_pred_early_st):4.3f}')\n",
    "print(f'Matthews correlation coefficient for plain Vanilla MLP with hypertuning:\\t {matthews_corrcoef(Y_test, Y_pred_gs):4.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 7: Model precision, recall, f1 and Matthew's correlation coefficent``\n",
    "`A` Compare and interpret the results from the classifications reports between the early stopping and hypertuned model.<br>\n",
    "`B` Compare and interpret the results from Matthews correlation coefficient between the early stopping and hypertuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A The results from the classification report for the MLP with early stopping we can be interpreted as follows:`<br>\n",
    "#   * The Precision is lowest for class Train and Car. This means that relative many predictions for Train and Car by the model are wrong.`<br>\n",
    "#   * Also the Recall is comparatively low for Train and Car. This means that the model relatively often fails to predict Car and Train cases as such.`\n",
    "#   * Comparing the two classifications reports, we can conclude that the MLP with the tuned hyperparameters does considerably better in terms of overall accuracy, butmore importantly in terms of the precision and recall Car and Train.`\n",
    "# B Matthews support the notion that the MLP with the tuned hyperparameters performs considerably better than the MLP with early stopping.`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit ('tester')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "68f115ea61190fc64bf12a5366d42cbcfd5b1392649f85812591ec193b7c9a2b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
