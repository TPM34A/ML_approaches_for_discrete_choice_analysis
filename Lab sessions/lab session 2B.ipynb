{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Choice Analysis: micro-econometrics and machine learning approaches\n",
    "\n",
    "## `Lab session 2B`<br> `Behavioural insights: Using SHAP values to improve model specifications`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q2 2022**<br>\n",
    "**Instructor:** Sander van Cranenburgh <br>\n",
    "**TAs:**  Francisco Garrido Valenzuela & Lucas Spierenburg <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Instructions`\n",
    "\n",
    "**Lab sessions aim to:**<br>\n",
    "* Show and reinforce how models and ideas presented in class are put to practice.<br>\n",
    "* Help you gather hands-on machine learning skills.<br>\n",
    "\n",
    "**Lab sessions are:**<br>\n",
    "* Learning environments where you work with Jupyter notebooks and where you can get support from TAs and fellow students.<br> \n",
    "* Not graded and do not have to be submitted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Workspace set-up`\n",
    "**Option 1: Google Colab**<br>\n",
    "Uncomment the following cell if you are running this notebook on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/TPM34A/ML_approaches_for_discrete_choice_analysis\n",
    "#!pip install -r ML_approaches_for_discrete_choice_analysis/requirements_colab.txt\n",
    "#!mv \"/content/ML_approaches_for_discrete_choice_analysis/Lab sessions/data\" /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Local environment**<br>\n",
    "Uncomment the following cell if you are running this notebook on your local environment, to install all dependencies on your Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Application: Swiss mode choice` <br>\n",
    "In this lab session we will continue analysing mode choices behaviour. However, now our are aim is to obtain behavioural insights from ML models. <br>\n",
    "To do so, in this lab session you will (1) develop a hybrid ANN-MNL model, and use (2) SHAP values <br>\n",
    "\n",
    "**Learning objectives**. After completing the following exercises you will be able to: <br>\n",
    "1. Estimate a  RUM-MNL model using PandasBiogeme (`lab session 2A`)<br>\n",
    "2. Train hybrid-ANN-MNL models and extract behavioural insights, such as VTTs, from them (`lab session 2A`)<br>\n",
    "3. Discuss the strength and weaknesses of using fully transparant RUM models, hybrid-models and fully opaque ANNs (`lab session 2A`)<br>\n",
    "4. **Use SHAP values to obtain behavioural insights from an otherwise opaque ML model**<br>\n",
    "5. **Use SHAP values to improve the model specification of a theory-driven RUM-MNL discrete choice model**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Organisation`\n",
    "This lab session comprises **`5`** parts:\n",
    "1. Preparing your data set\n",
    "2. Training a XGBoost classifier\n",
    "3. Evaluating model performance\n",
    "4. Using SHAP values to get insights on global feature importances\n",
    "5. Using XAI insights to improve model specifications theory-driven discrete choice models\n",
    "\n",
    "and comprises **`2`** exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python packages and modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import getcwd\n",
    "\n",
    "# Import shap module\n",
    "import shap\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, log_loss, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# Import XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import Biogeme\n",
    "import biogeme.biogeme as bio\n",
    "import biogeme.database as db\n",
    "import biogeme.optimization as opt\n",
    "import biogeme.messaging as msg\n",
    "from biogeme import models\n",
    "from biogeme.expressions import Beta\n",
    "\n",
    "# Setting\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Preparing your data set**\n",
    "To prepare the data set, we will:<br>\n",
    "i. **Load** the data set<br>\n",
    "ii. **Inspect** and **Clean** the data set<br>\n",
    "iii. **Discover and visualise** the data <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "i) Load the choice data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "datafile_path = os.path.join(getcwd(),'data','')\n",
    "print(datafile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mode choice data into a pandas DataFrame\n",
    "df = pd.read_csv(datafile_path + 'swissmetro.dat',sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ii) Clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "# Only keep data for purposes 'Commute\" and \"Business\"\n",
    "df.drop(df[(df.PURPOSE != 1) & (df.PURPOSE != 3)].index, inplace=True) \n",
    "\n",
    "# Drop rows with unknown choices (CHOICE == 0)\n",
    "df.drop(df[df.CHOICE == 0].index, inplace=True) \n",
    "\n",
    "# In case of reamining missing values, replace them with 0\n",
    "df.fillna(0, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the data into a binary choice**\n",
    "SHAP values are particularly good interpretable for binary choice data (i.e. 2 target classes)<br>\n",
    "Earlier analysis showed that the RUM-MNL model particularly was doing a poor job in predicting Train. The Recall of the MNL model for train was very low.<br>\n",
    "We are going to use SHAP to improve this. Therefore, we drop all observations with CHOICE == 'Car' <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep data for TRAIN and SM\n",
    "df2 = df.copy()\n",
    "df2 = df2.drop(df2[df2.CHOICE == 3].index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii) Splitting the data in a train set and a test set**<br>\n",
    "Training ML models always involves a train and a test data set. The former is used to update the weights of the model. Tha latter is used to evaluate the generalisation performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X variable containing the features\n",
    "features = ['PURPOSE', 'FIRST', \n",
    "            'TICKET', 'WHO', 'LUGGAGE', 'AGE', \n",
    "            'MALE', 'INCOME', 'GA', 'ORIGIN', \n",
    "            'DEST', 'TRAIN_TT', 'TRAIN_CO', \n",
    "            'TRAIN_HE', 'SM_TT', 'SM_CO', \n",
    "            'SM_HE', 'SM_SEATS']\n",
    "X = df2[features]            \n",
    "\n",
    "# Create the target [0,1]\n",
    "# 0 = TRAIN\n",
    "# 1 = SM\n",
    "Y = df2['CHOICE']-1\n",
    "\n",
    "# Split the data using sk-learn's `train_test_split` function\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 12345, test_size = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Training a XGBoost classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SHAP is model agnostic we can use it any ML model. In this lab session we use an **XGBoost model**. The reason for using XGBoost instead of an ANN is because it gives more illustrative results, better aligned with the educational purposes. However, if you have time, you can also replace the XGBoost model with an MLP, and see yourself how it performs then."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a XGBoost classifier object using the specified (optimised) hyperparameters\n",
    "# Note that for this XGBoost model we do not need to scale the data\n",
    "model = XGBClassifier(eval_metric = 'logloss',colsample_bytree = 0.7,gamma=1.5,max_depth=4,min_child_weight=1,subsample=1)\n",
    "\n",
    "# Train the XGBoost model\n",
    "model.fit(X_train, Y_train, eval_set = [(X_train, Y_train), (X_test, Y_test)],verbose=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot learning curves\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "results = model.evals_result()\n",
    "plt.plot(results['validation_0']['logloss'], label='train')\n",
    "plt.plot(results['validation_1']['logloss'], label='test')\n",
    "plt.legend()\n",
    "ax.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Evaluating model performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the choices for the test data set, using the model\n",
    "Y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Show the confusion matrices, to asses improvements compared to the non hyperparameter tuned network\n",
    "fig, ax = plt.subplots(1,2,figsize = (16,6))\n",
    "# fig.set_tight_layout(True)\n",
    "\n",
    "cm1 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_test, display_labels = ['Train', 'SM'], normalize=None,  ax=ax[0])\n",
    "cm2 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_test, display_labels = ['Train', 'SM'], normalize='true',ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the precision, recal and f1 score we conveniently use sk-learn's 'classification_report' functionality\n",
    "print('Classification report forthe RUM-MNL model\\n',\n",
    "    classification_report(Y_test,Y_pred_test, target_names= ['Train', 'SM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Using SHAP values to get insights on global feature importances**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute SHAP values for the shap sample\n",
    "explainer = shap.Explainer(model, X)\n",
    "shap_values = explainer(X)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global feature importance\n",
    "shap.plots.bar(shap_values,order=shap_values.abs.max(0), max_display=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partial dependecies plots\n",
    "This scatter plot shows the effect a single feature has on the predictions made by the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MALE (0: female, 1: male)\n",
    "shap.plots.scatter(shap_values[:,\"MALE\"])\n",
    "shap.plots.scatter(shap_values[:,\"INCOME\"])\n",
    "\n",
    "# Interpretation MALE:\n",
    "# The partial dependency plot shows Females are have a negative SHAP value. The SHAP value represents how much knowing that feature’s value changes the output of the model, and in what direction.\n",
    "# Because the SHAP value for female is enequal to zero, it suggests the feature MALE has explanatory power in the prediction of the mode choice.\n",
    "# As the SHAP value is negative for a female, knowing the traveller is female decreases the probability of choosing SM\n",
    "\n",
    "# Interpretation INCOME:\n",
    "# The partial dependency plot shows income level 4 has a negative SHAP value. The SHAP value represents how much knowing that feature’s value changes the output of the model, and in what direction.\n",
    "# Because the SHAP value for income level 4 is enequal to zero, it suggests the feature INCOME has explanatory power in the prediction of the mode choice.\n",
    "# As the SHAP value is negative for income level 4, knowing the income is decreases the probability of choosing SM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Exercise 1: Partial dependecy plots`<br>\n",
    "`A` Create dependency plots for other features that have an impact on the choices<br>\n",
    "`B` Interpret these dependency plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here\n",
    "shap.plots.scatter(shap_values[:,\"TICKET\"])\n",
    "shap.plots.scatter(shap_values[:,\"PURPOSE\"])\n",
    "shap.plots.scatter(shap_values[:,\"AGE\"])\n",
    "shap.plots.scatter(shap_values[:,\"MALE\"])\n",
    "shap.plots.scatter(shap_values[:,\"DEST\"])\n",
    "shap.plots.scatter(shap_values[:,\"AGE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Using XAI insights to improve model specifications theory-driven discrete choice models**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Exercise 2:`**` COMPETITION`** <br> `Improving the model specification of the theory-driven RUM-MNL discrete choice model`<br>\n",
    "Use the behavioural insights obtain from your SHAP analysis AND your choice modelling expertise to improve the model specification / performance of the RUM-MNL discrete choice model<br>\n",
    "Your model is evaluated based on two criterion:\n",
    "1. BIC value\n",
    "2. Value-of-Travel-Time (VTT)\n",
    "The winning model is the one with the lowest BIC value & for which holds 10<VTT<100< chf/hr <br>\n",
    "\n",
    "`The winner obtains a prize, and eternal recognition ;)`\n",
    "\n",
    "Note 1: In case you use alternative specific betas for time, then the VTT condition much hold for all modes.<br>\n",
    "Note 2: Do not use alternative specific betas for cost, as this violates economic rationale.<br>\n",
    "Note 3: Only RUM-MNL specifications are accepted (thus, mixture models, such as Mixed Logit, or Latent Class models are not admissible)<br>\n",
    "Note 4: I managed to get to a BIC value of 10,189 and a cross entropy of 0.74 (in 30 min tweaking). However, in the previous lab session we have seen that the hybrid ANN-MNL and the fully connected ANN both attain a considerably better cross entropy of 0.70 and 0.64, respectively. Therefore, we know there is considerable scope to further improve this specification!<br>\n",
    "\n",
    "For your convenience, below first the baseline model is provided. Copy this model to a new cell under it, and start gradually improving the model specification.\n",
    "**Good luck!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas df in biogeme database\n",
    "database = db.Database('swissmetro data', df)\n",
    "\n",
    "# The following statement allows you to use the names of the variable as Python variables.\n",
    "globals().update(database.variables)\n",
    "\n",
    "# Parameters to be estimated\n",
    "ASC_CAR = Beta('ASC_CAR', 0, None, None, 0)\n",
    "ASC_TRAIN = Beta('ASC_TRAIN', 0, None, None, 1)\n",
    "ASC_SM = Beta('ASC_SM', 0, None, None, 0)\n",
    "B_TIME = Beta('B_TIME', 0, None, None, 0)\n",
    "B_COST = Beta('B_COST', 0, None, None, 0)\n",
    "\n",
    "# Set cost to zero for concession card holders\n",
    "SM_COST = SM_CO * (GA == 0)             \n",
    "TRAIN_COST = TRAIN_CO * (GA == 0)       \n",
    "\n",
    "# Rescale attributes for numerical stability\n",
    "TRAIN_TT_SCALED   = TRAIN_TT / 100\n",
    "TRAIN_COST_SCALED = TRAIN_COST / 100\n",
    "SM_TT_SCALED      = SM_TT / 100\n",
    "SM_COST_SCALED    = SM_COST / 100\n",
    "CAR_TT_SCALED     = CAR_TT / 100\n",
    "CAR_CO_SCALED     = CAR_CO / 100\n",
    "\n",
    "# Utility functions\n",
    "V1 = ASC_TRAIN + B_TIME * TRAIN_TT_SCALED + B_COST * TRAIN_COST_SCALED\n",
    "V2 = ASC_SM    + B_TIME * SM_TT_SCALED    + B_COST * SM_COST_SCALED\n",
    "V3 = ASC_CAR   + B_TIME * CAR_TT_SCALED   + B_COST * CAR_CO_SCALED\n",
    "\n",
    "# Associate utility functions with the numbering of alternatives in df.CHOICE\n",
    "V = {1: V1, 2: V2, 3: V3}\n",
    "\n",
    "# Associate the availability conditions with the alternatives\n",
    "AV = {1: TRAIN_AV, 2: SM_AV, 3: CAR_AV}\n",
    "\n",
    "# Definition of the model. This is the contribution of each observation to the log likelihood function.\n",
    "logprob = models.loglogit(V, AV, CHOICE)\n",
    "\n",
    "# Create the Biogeme object\n",
    "biogeme = bio.BIOGEME(database, logprob)\n",
    "biogeme.modelName = 'RUM-MNL model with TRAIN and SM'\n",
    "biogeme.generatePickle = False\n",
    "biogeme.generateHtml = False\n",
    "\n",
    "# Calculate the null log likelihood for reporting.\n",
    "biogeme.calculateNullLoglikelihood(AV)\n",
    "\n",
    "# Estimate the parameters\n",
    "results = biogeme.estimate()\n",
    "\n",
    "# Report the results in a pandas table\n",
    "print('Estimated parameters')\n",
    "print('----------')\n",
    "print(results.getEstimatedParameters()[['Value','Std err','t-test','p-value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the model performance\n",
    "print(results.shortSummary())\n",
    "\n",
    "# Compute and print the cross entropy\n",
    "cross_entropy =  -(results.getGeneralStatistics()['Final log likelihood'][0])/(results.getGeneralStatistics()['Sample size'][0])\n",
    "print(f'Cross entropy:\\t\\t\\t {cross_entropy:.3f}')\n",
    "\n",
    "# Compute the Value-of-Travel Time: (60*beta_TIME/beta_COST)\n",
    "betas = results.getBetaValues()\n",
    "VTT   = 60*betas['B_TIME']/(betas['B_COST'])\n",
    "print(f'\\nThe Value-of-Travel-Time = {VTT:.3f} chf/hr.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 64-bit ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "52edd5821628e65ed257fae09420cd5b7ee9f38a64f5e5ba5a5e47a3545ddf85"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
