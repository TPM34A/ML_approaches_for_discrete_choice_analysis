{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Choice Analysis: micro-econometrics and machine learning approaches\n",
    "\n",
    "## `Lab session 2B`<br> `Behavioural insights: Using SHAP values to improve model specifications`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**July 2022**<br>\n",
    "**Instructor:** Sander van Cranenburgh <br>\n",
    "**TAs:**  Francisco Garrido Valenzuela & Lucas Spierenburg <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Instructions`\n",
    "\n",
    "**Lab sessions aim to:**<br>\n",
    "* Show and reinforce how models and ideas presented in class are put to practice.<br>\n",
    "* Help you gather hands-on machine learning skills.<br>\n",
    "\n",
    "**Lab sessions are:**<br>\n",
    "* Learning environments where you work with Jupyter notebooks and where you can get support from TAs and fellow students.<br> \n",
    "* Not graded and do not have to be submitted. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Workspace set-up`\n",
    "**Option 1: Google Colab**<br>\n",
    "Uncomment the following cell if you are running this notebook on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!rm -rf /content/ML_approaches_for_discrete_choice_analysis\n",
    "#!git clone https://github.com/TPM34A/ML_approaches_for_discrete_choice_analysis\n",
    "#!pip install -r ML_approaches_for_discrete_choice_analysis/requirements_colab.txt\n",
    "#!cp -R \"/content/ML_approaches_for_discrete_choice_analysis/Lab sessions/data\" /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Local environment**<br>\n",
    "Uncomment the following cell if you are running this notebook on your local environment, to install all dependencies on your Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements_local.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Application: Swiss mode choice` <br>\n",
    "In this lab session we will continue analysing mode choices behaviour. However, now our are aim is to obtain behavioural insights from ML models. <br>\n",
    "To do so, in this lab session you will (1) develop a hybrid ANN-MNL model, and use (2) SHAP values <br>\n",
    "\n",
    "**Learning objectives**. After completing the following exercises you will be able to: <br>\n",
    "1. Estimate a  RUM-MNL model using PandasBiogeme (`lab session 2A`)<br>\n",
    "2. Train hybrid-ANN-MNL models and extract behavioural insights, such as VTTs, from them (`lab session 2A`)<br>\n",
    "3. Discuss the strength and weaknesses of using fully transparant RUM models, hybrid-models and fully opaque ANNs (`lab session 2A`)<br>\n",
    "4. **Use SHAP values to obtain behavioural insights from an otherwise opaque ML model**<br>\n",
    "5. **Use SHAP values to improve the model specification of a theory-driven RUM-MNL discrete choice model**<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Organisation`\n",
    "This lab session comprises **`5`** parts:\n",
    "1. Preparing your data set\n",
    "2. Training a XGBoost classifier\n",
    "3. Evaluating model's pridiction performance\n",
    "4. Using SHAP values to get insights on global feature importances\n",
    "5. Using XAI insights to improve model specifications theory-driven discrete choice models\n",
    "\n",
    "and comprises **`2`** exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python packages and modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from os import getcwd\n",
    "\n",
    "# Import shap module\n",
    "import shap\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, log_loss, ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# Import XGBoost\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Import Biogeme\n",
    "import biogeme.biogeme as bio\n",
    "import biogeme.database as db\n",
    "import biogeme.optimization as opt\n",
    "import biogeme.messaging as msg\n",
    "from biogeme import models\n",
    "from biogeme.expressions import Beta\n",
    "\n",
    "# Setting\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Preparing your data set**\n",
    "To prepare the data set, we will:<br>\n",
    "i. **Load** the data set<br>\n",
    "ii. **Clean** the data and **convert** into a **binary choice** problem<br>\n",
    "iii. **Splitting** the data in a train set and a test set the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**i) Load the choice data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "datafile_path = os.path.join(getcwd(),'data','')\n",
    "print(datafile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mode choice data into a pandas DataFrame\n",
    "df = pd.read_csv(datafile_path + 'swissmetro.dat',sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ii) Clean the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "# Only keep data for purposes 'Commute\" and \"Business\"\n",
    "df.drop(df[(df.PURPOSE != 1) & (df.PURPOSE != 3)].index, inplace=True) \n",
    "\n",
    "# Drop rows with unknown choices (CHOICE == 0)\n",
    "df.drop(df[df.CHOICE == 0].index, inplace=True) \n",
    "\n",
    "# In case of reamining missing values, replace them with 0\n",
    "df.fillna(0, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convert the data into a binary choice**<br>\n",
    "SHAP values are particularly good interpretable for binary choice data (i.e. 2 target classes)<br>\n",
    "Earlier analysis showed that the RUM-MNL model particularly was doing a poor job in predicting Train. The Recall of the MNL model for train was very low.<br>\n",
    "We are going to use SHAP to improve this. Therefore, we drop all observations with CHOICE == 'Car' <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep data for TRAIN and SM\n",
    "# The dataframe with only the 2 alternatives is 'df2'\n",
    "df2 = df.copy()\n",
    "df2 = df2.drop(df2[df2.CHOICE == 3].index) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**iii) Splitting the data in a train set and a test set**<br>\n",
    "Training ML models always involves a train and a test data set. The former is used to update the weights of the model. Tha latter is used to evaluate the generalisation performance of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create X variable containing the features\n",
    "# The features of CAR are not considered of importance\n",
    "features = ['PURPOSE', 'FIRST',\n",
    "            'TICKET', 'WHO', 'LUGGAGE', 'AGE', \n",
    "            'MALE', 'INCOME', 'GA', 'ORIGIN', \n",
    "            'DEST', 'TRAIN_TT', 'TRAIN_CO', \n",
    "            'TRAIN_HE', 'SM_TT', 'SM_CO', \n",
    "            'SM_HE', 'SM_SEATS']\n",
    "X = df2[features]            \n",
    "\n",
    "# Create the target [0,1]\n",
    "# 0 = TRAIN\n",
    "# 1 = SM\n",
    "Y = df2['CHOICE']-1\n",
    "\n",
    "# Split the data using sk-learn's `train_test_split` function\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, random_state = 12345, test_size = 0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Training a XGBoost classifier**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since SHAP is model agnostic we can use it any ML model. In this lab session we use an **XGBoost model**. The reason for using XGBoost instead of an ANN is because it gives more illustrative (and better) results, aligning with the educational purpose of this lab session. However, if you have time, you can also replace the XGBoost model with an MLP, and see yourself how it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a XGBoost classifier object using the specified hyperparameters\n",
    "# Note that for this XGBoost model we do not need to scale the data\n",
    "# Note the hyperparameter are optimised using a GridSearchCV\n",
    "# Note you can ignore the meaning of the hyperparameters. We only need the trained classifier.\n",
    "model = XGBClassifier(eval_metric = 'logloss',colsample_bytree = 0.7, gamma=1.5, max_depth=4, min_child_weight=1, subsample=1)\n",
    "\n",
    "# Train the XGBoost model\n",
    "model.fit(X_train, Y_train, eval_set = [(X_train, Y_train), (X_test, Y_test)],verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. Evaluating model's pridiction performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the choices for the test data set, using the model\n",
    "Y_pred_test = model.predict(X_test)\n",
    "\n",
    "# Show the confusion matrices (non-normalised & normalised)\n",
    "fig, ax = plt.subplots(1,2,figsize = (16,6))\n",
    "cm1 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_test, display_labels = ['Train', 'SM'], normalize=None,  ax=ax[0])\n",
    "cm2 = ConfusionMatrixDisplay.from_predictions(y_true=Y_test,y_pred=Y_pred_test, display_labels = ['Train', 'SM'], normalize='true',ax=ax[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the precision, recal and f1 score we use sk-learn's 'classification_report' functionality\n",
    "print('Classification report forthe RUM-MNL model\\n',\n",
    "    classification_report(Y_test,Y_pred_test, target_names= ['Train', 'SM']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Using SHAP values to get insights on global feature importances**\n",
    "Next, we are going to compute the SHAP values. To do so, we create a shap explainer object, and provide it with a trained model, and the instances to explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the shap explainer object\n",
    "explainer = shap.Explainer(model, X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the explainer to instance(s) X\n",
    "shap_values = explainer(X)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Visualising SHAP values**\n",
    "There are several ways to **visualise** shap explanations. See [the shape documentation website](https://shap.readthedocs.io/en/latest/index.html) for an excellent overview of the use and visualisation of shap values.<br>\n",
    "Below, we will use the following 3 visualisations: <br> \n",
    "1. Bar plot\n",
    "2. Summary plot\n",
    "3. Dependence scatter plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bar plot**<br>\n",
    "The bar plot is the simplest way to visualise feature importances obtained from the shap explainer. Bar plots can be used both for global and local explanations. A larger bar means that knowing the features value has a larger impact on the model's prediction (hence, is more important)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the feature importances obtained from the shap explainer applied to X\n",
    "# The plot shows how much knowing that feature’s value changes the output of the model prediction\n",
    "# Since here we use the full data set (X). The importances show below are global importances. The shap values are avaraged across the instances in X. \n",
    "shap.plots.bar(shap_values, max_display=18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summary plot**<br>\n",
    "Another insightful - but more complex - way to visualise the shap explanations is using the so-called shap summary plot. This plot shows a density scatter plot of shap values for each feature. Each dot represents a shap value explaining an instance of X. What is particularly insightful in this plot is that the color representing the feature value shows the direction of the expected impact on the model's prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.summary_plot(shap_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{blue}{\\text{In the plot above, the top row shows that a low value of SM\\_COST (blue) increase the prediction in the direction of SM.}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dependence scatter plot**<br>\n",
    "A dependence scatter plot shows the effect a single feature has on the predictions made by the model. Each dot represents an instance in X.\n",
    "On the y-axis the shap value is depicted. It shows how much knowing that feature’s value changes the output of the model for an instance in X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MALE (0: female, 1: male)\n",
    "shap.plots.scatter(shap_values[:,\"MALE\"])\n",
    "shap.plots.scatter(shap_values[:,\"INCOME\"])\n",
    "\n",
    "# Interpretation MALE:\n",
    "# The partial dependency plot shows Females (0) have a negative SHAP value. The SHAP value represents how much knowing that feature’s value changes the output of the model, and in what direction.\n",
    "# Because the SHAP value for female is enequal to zero, it suggests the feature MALE has explanatory power in the prediction of the mode choice.\n",
    "# As the SHAP value is negative for a female, knowing the traveller is female decreases the probability of choosing SM\n",
    "# Note that MALE is an categorical variable, taking the values {0,1}. In the plot the values seems to deviate on the x-axis. This is only done to improve the visualisation.\n",
    "\n",
    "# Interpretation INCOME:\n",
    "# The partial dependency plot shows income level 4 has a negative SHAP value. The SHAP value represents how much knowing that feature’s value changes the output of the model, and in what direction.\n",
    "# Because the SHAP value for income level 4 is enequal to zero, it suggests the feature INCOME has explanatory power in the prediction of the mode choice.\n",
    "# As the SHAP value is negative for income level 4, knowing the income is decreases the probability of choosing SM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Exercise 1: Dependency plots`<br>\n",
    "`A` Create dependency plots for other features that are found to impact on the mode choice behaviour<br>\n",
    "`B` Interpret these dependency plots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Add your answers here}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ANSWERS\n",
    "# A)\n",
    "# B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **5. Using XAI insights to improve model specifications theory-driven discrete choice models**\n",
    "One interesting way to use XAI is to use it to improve the model specification of theory-driven discrete choice models. In particular, a choice modeller can use the features that are discovered to be of importance to the ML model to develop a good theory-driven choice model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Exercise 2`\n",
    "#### **` COMPETITION - Improving the model specification of the theory-driven RUM-MNL discrete choice model`**\n",
    "Use the behavioural insights obtain from your SHAP analysis AND your choice modelling expertise to improve the model specification / performance of the RUM-MNL discrete choice model. Your model is evaluated based on two criterion:\n",
    "1. BIC value:<br><br> $BIC = -2\\cdot LL + k\\cdot ln(N)$, where *N* is the number of observations and *k* the number of estimated parameters.<br><br>\n",
    "2. Value-of-Travel-Time:<br><br> $VTT = -60\\cdot\\frac{\\beta_{TIME}}{\\beta_{COST}}$<br><br>\n",
    "The **winning model** is the one with the **lowest BIC value** & for which holds **10<VTT<100** Chf/hr <br>\n",
    "\n",
    "**`The winner obtains a prize, and eternal recognition ;)`**\n",
    "\n",
    "Note 1: In case you use alternative specific betas for time, then the VTT condition much hold for all modes.<br>\n",
    "Note 2: Do not use alternative specific betas for cost, as this violates economic rationale.<br>\n",
    "Note 3: Only RUM-MNL specifications are accepted (thus, mixture models, such as Mixed Logit, or Latent Class models are not admissible)<br>\n",
    "Note 4: I managed to get to a BIC value of 10,189 and a cross entropy of 0.74 (in 30 minutes of model development). However, in the previous lab session we have seen that the hybrid ANN-MNL and the fully connected ANN both attain a considerably better cross entropy of 0.70 and 0.64, respectively. Therefore, we know there is considerable scope to further improve this specification!<br>\n",
    "\n",
    "For your convenience, below first the baseline model is provided. Copy this model to a new cell under it, and start gradually improving the model specification.<br>\n",
    "**GOOD LUCK!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Baseline model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas df in biogeme database\n",
    "database = db.Database('swissmetro data', df)\n",
    "\n",
    "# The following statement allows you to use the names of the variable as Python variables.\n",
    "globals().update(database.variables)\n",
    "\n",
    "# Parameters to be estimated\n",
    "ASC_CAR = Beta('ASC_CAR', 0, None, None, 0)\n",
    "ASC_TRAIN = Beta('ASC_TRAIN', 0, None, None, 1)\n",
    "ASC_SM = Beta('ASC_SM', 0, None, None, 0)\n",
    "B_TIME = Beta('B_TIME', 0, None, None, 0)\n",
    "B_COST = Beta('B_COST', 0, None, None, 0)\n",
    "\n",
    "# Set cost to zero for concession card holders\n",
    "SM_COST = SM_CO * (GA == 0)             \n",
    "TRAIN_COST = TRAIN_CO * (GA == 0)       \n",
    "\n",
    "# Rescale attributes for numerical stability\n",
    "TRAIN_TT_SCALED   = TRAIN_TT / 100\n",
    "TRAIN_COST_SCALED = TRAIN_COST / 100\n",
    "SM_TT_SCALED      = SM_TT / 100\n",
    "SM_COST_SCALED    = SM_COST / 100\n",
    "CAR_TT_SCALED     = CAR_TT / 100\n",
    "CAR_CO_SCALED     = CAR_CO / 100\n",
    "\n",
    "# Utility functions\n",
    "V1 = ASC_TRAIN + B_TIME * TRAIN_TT_SCALED + B_COST * TRAIN_COST_SCALED\n",
    "V2 = ASC_SM    + B_TIME * SM_TT_SCALED    + B_COST * SM_COST_SCALED\n",
    "V3 = ASC_CAR   + B_TIME * CAR_TT_SCALED   + B_COST * CAR_CO_SCALED\n",
    "\n",
    "# Associate utility functions with the numbering of alternatives in df.CHOICE\n",
    "V = {1: V1, 2: V2, 3: V3}\n",
    "\n",
    "# Associate the availability conditions with the alternatives\n",
    "AV = {1: TRAIN_AV, 2: SM_AV, 3: CAR_AV}\n",
    "\n",
    "# Definition of the model. This is the contribution of each observation to the log likelihood function.\n",
    "logprob = models.loglogit(V, AV, CHOICE)\n",
    "\n",
    "# Create the Biogeme object\n",
    "biogeme = bio.BIOGEME(database, logprob)\n",
    "biogeme.modelName = 'RUM-MNL model with TRAIN and SM'\n",
    "biogeme.generatePickle = False\n",
    "biogeme.generateHtml = False\n",
    "\n",
    "# Calculate the null log likelihood for reporting.\n",
    "biogeme.calculateNullLoglikelihood(AV)\n",
    "\n",
    "# Estimate the parameters\n",
    "results = biogeme.estimate()\n",
    "\n",
    "# Report the results in a pandas table\n",
    "print('Estimated parameters')\n",
    "print('----------')\n",
    "print(results.getEstimatedParameters()[['Value','Std err','t-test','p-value']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the model performance\n",
    "print(results.shortSummary())\n",
    "\n",
    "# Compute and print the cross entropy\n",
    "cross_entropy =  -(results.getGeneralStatistics()['Final log likelihood'][0])/(results.getGeneralStatistics()['Sample size'][0])\n",
    "print(f'Cross entropy:\\t\\t\\t {cross_entropy:.3f}')\n",
    "\n",
    "# Compute the Value-of-Travel Time: (60*beta_TIME/beta_COST)\n",
    "betas = results.getBetaValues()\n",
    "VTT   = 60*betas['B_TIME']/(betas['B_COST'])\n",
    "print(f'\\nThe Value-of-Travel-Time = {VTT:.3f} chf/hr.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\color{green}{\\text{Add your answers here}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
