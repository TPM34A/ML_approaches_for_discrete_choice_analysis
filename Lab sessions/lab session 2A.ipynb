{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discrete Choice Analysis: micro-econometrics and machine learning approaches\n",
    "\n",
    "## `Lab session 2A`<br> `Behavioural insights: Hybrid ANN-MNL models`\n",
    "\n",
    "**Delft University of Technology**<br>\n",
    "**Q2 2022**<br>\n",
    "**Instructor:** Sander van Cranenburgh <br>\n",
    "**TAs:**  Francisco Garrido Valenzuela & Lucas Spierenburg <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Workspace set-up`\n",
    "**Option 1: Google Colab**<br>\n",
    "Uncomment the following cell if you are running this notebook on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!git clone https://github.com/TPM34A/ML_approaches_for_discrete_choice_analysis\n",
    "#!pip install -r ML_approaches_for_discrete_choice_analysis/requirements.txt\n",
    "#!mv \"/content/ML_approaches_for_discrete_choice_analysis/Lab sessions/data\" /content/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Option 2: Local environment**<br>\n",
    "Uncomment the following cell if you are running this notebook on your local environment, to install all dependencies on your Python version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Instructions`\n",
    "\n",
    "**Lab sessions aim to:**<br>\n",
    "* Show and reinforce how models and ideas presented in class are put to practice.<br>\n",
    "* Help you gather hands-on machine learning skills.<br>\n",
    "\n",
    "**Lab sessions are:**<br>\n",
    "* Learning environments where you work with Jupyter notebooks and where you can get support from TAs and fellow students.<br> \n",
    "* Not graded and do not have to be submitted. \n",
    "<br><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `Application: Swiss mode choice` <br>\n",
    "In this lab session we will continue analysing mode choices behaviour. However, now our are aim is to obtain behavioural insights from ML models. <br>\n",
    "To do so, in this lab session you will (1) develop a hybrid ANN-MNL model, and use (2) SHAP values <br>\n",
    "\n",
    "**Learning objectives**. After completing the following exercises you will be able to: <br>\n",
    "1. Estimate a  RUM-MNL model using PandasBiogeme<br>\n",
    "2. Train hybrid-ANN-MNL models and extract behavioural insights, such as VTTs, from them<br>\n",
    "3. Discuss the strength and weaknesses of using fully transparant RUM models, hybrid-models and fully opaque ANNs <br>\n",
    "4. Use SHAP values to obtain behavioural insights from an otherwise opaque ML model (separate notebook)<br>\n",
    "5. Use SHAP values to improve the model specification of a theory-driven RUM-MNL discrete choice model (separate notebook)<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Organisation`\n",
    "This lab session comprises **`6`** parts:\n",
    "1. Preparing your data set\n",
    "2. Estimating a RUM-MNL discrete choice model using PandasBiogeme (to benchmark)\n",
    "3. The hybrid ANN-MNL model\n",
    "4. Evaluating and comparing performances of trained models\n",
    "\n",
    "and comprises **`4`** exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required Python packages and modules\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from os import getcwd\n",
    "\n",
    "# Biogeme\n",
    "import biogeme.biogeme as bio\n",
    "import biogeme.database as db\n",
    "import biogeme.optimization as opt\n",
    "import biogeme.messaging as msg\n",
    "from biogeme import models\n",
    "from biogeme.expressions import Beta\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, classification_report\n",
    "\n",
    "# Tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import backend as K\n",
    "from keras.layers import Input, Dense, Activation\n",
    "from keras.layers import Conv2D, Add, Reshape\n",
    "from keras.models import Model\n",
    "from keras.utils.np_utils import to_categorical   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Preparing your data set** <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Load the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the current working directory\n",
    "datafile_path = os.path.join(getcwd(),'data','')\n",
    "print(datafile_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load mode choice data into a pandas DataFrame\n",
    "df = pd.read_csv(datafile_path + 'swissmetro.dat',sep = '\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Clean the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data cleaning\n",
    "\n",
    "# Only keep data for purposes 'Commute\" and \"Business\"\n",
    "df.drop(df[(df.PURPOSE != 1) & (df.PURPOSE != 3)].index, inplace=True) \n",
    "\n",
    "# Drop rows with unknown choices (CHOICE == 0)\n",
    "df.drop(df[df.CHOICE == 0].index, inplace=True) \n",
    "\n",
    "# In case of reamining missing values, replace them with 0\n",
    "df.fillna(0, inplace = True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Correct cost for concession card holders**\n",
    "When travellers have a concession card, the marginal cost for a trip is zero. That is, one extra trip does not cost anything extra. <br>\n",
    "Therefore, we 'manually' set the TRAIN and SM cost to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When GA equals 0, the traveller does not have a concession card; when GA equals 1, the traveller has a concession card. \n",
    "df['SM_CO'] = df.SM_CO * (df.GA == 0)             \n",
    "df['TRAIN_CO'] = df.TRAIN_CO * (df.GA == 0)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Estimating a RUM-MNL discrete choice model using PandasBiogeme (to benchmark)**\n",
    "To compare the performance and outputs of the hybrid ANN-MNL that we will build next, we first estimate our benchmark model: the canonical RUM-MNL.<br>\n",
    "\n",
    "\n",
    "In the RUM-MNL model utility is assumed to be linear additive-utility: \n",
    "\n",
    "$ V_{in} = ASC_{i} + \\sum_{m}\\beta_m x_{imn}$\n",
    "\n",
    "With this model we estimate the ASCs and marginal utilities (i.e. betas) for: \n",
    "\n",
    "1. Travel Time [min] \n",
    "2. Travel cost [chf] (Swiss franc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pandas df in biogeme database\n",
    "database = db.Database('swissmetro data', df)\n",
    "\n",
    "# The following statement allows you to use the names of the variable as Python variables.\n",
    "globals().update(database.variables)\n",
    "\n",
    "# Parameters to be estimated\n",
    "ASC_CAR = Beta('ASC_CAR', 0, None, None, 0)\n",
    "ASC_TRAIN = Beta('ASC_TRAIN', 0, None, None, 1)\n",
    "ASC_SM = Beta('ASC_SM', 0, None, None, 0)\n",
    "B_TIME = Beta('B_TIME', 0, None, None, 0)\n",
    "B_COST = Beta('B_COST', 0, None, None, 0)\n",
    "\n",
    "# Set cost to zero for concession card holders\n",
    "SM_COST = SM_CO * (GA == 0)             \n",
    "TRAIN_COST = TRAIN_CO * (GA == 0)       \n",
    "\n",
    "# Rescale attributes for numerical stability\n",
    "TRAIN_TT_SCALED   = TRAIN_TT / 100\n",
    "TRAIN_COST_SCALED = TRAIN_COST / 100\n",
    "SM_TT_SCALED      = SM_TT / 100\n",
    "SM_COST_SCALED    = SM_COST / 100\n",
    "CAR_TT_SCALED     = CAR_TT / 100\n",
    "CAR_CO_SCALED     = CAR_CO / 100\n",
    "\n",
    "# Utility functions\n",
    "V1 = ASC_TRAIN + B_TIME * TRAIN_TT_SCALED + B_COST * TRAIN_COST_SCALED\n",
    "V2 = ASC_SM    + B_TIME * SM_TT_SCALED    + B_COST * SM_COST_SCALED\n",
    "V3 = ASC_CAR   + B_TIME * CAR_TT_SCALED   + B_COST * CAR_CO_SCALED\n",
    "\n",
    "# Associate utility functions with the numbering of alternatives in df.CHOICE\n",
    "V = {1: V1, 2: V2, 3: V3}\n",
    "\n",
    "# Associate the availability conditions with the alternatives\n",
    "AV = {1: TRAIN_AV, 2: SM_AV, 3: CAR_AV}\n",
    "\n",
    "# Definition of the model. This is the contribution of each observation to the log likelihood function.\n",
    "logprob = models.loglogit(V, AV, CHOICE)\n",
    "\n",
    "# Create the Biogeme object\n",
    "biogeme = bio.BIOGEME(database, logprob)\n",
    "biogeme.modelName = 'RUM-MNL'\n",
    "biogeme.generatePickle = False\n",
    "biogeme.generateHtml = False\n",
    "\n",
    "# Calculate the null log likelihood for reporting.\n",
    "biogeme.calculateNullLoglikelihood(AV)\n",
    "\n",
    "# Estimate the parameters\n",
    "results = biogeme.estimate()\n",
    "\n",
    "# Report the results in a pandas table\n",
    "print('Estimated parameters')\n",
    "print('----------')\n",
    "print(results.getEstimatedParameters()[['Value','Std err','t-test','p-value']])\n",
    "\n",
    "# Perform a 'simulation' to compute the choice probabilities based on the estimated model (for later use)\n",
    "simulate = {\n",
    "    'Prob_TRAIN':      models.logit(V, AV, 1),\n",
    "    'Prob_SM':         models.logit(V, AV, 2),\n",
    "    'Prob_CAR':        models.logit(V, AV, 3)}\n",
    "\n",
    "# Create the biogeme simulation object\n",
    "biosim = bio.BIOGEME(database, simulate)\n",
    "prob_mnl = biosim.simulate(results.getBetaValues()).to_numpy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Show the model performance statistics and the  Value of Travel Time (VTT)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the model performance\n",
    "print(results.shortSummary())\n",
    "\n",
    "# Compute and print the cross entropy\n",
    "cross_entropy =  -(results.getGeneralStatistics()['Final log likelihood'][0])/(results.getGeneralStatistics()['Sample size'][0])\n",
    "print(f'Cross entropy:\\t\\t\\t {cross_entropy:.3f}')\n",
    "\n",
    "# Compute the Value-of-Travel Time: (60*beta_TIME/beta_COST)\n",
    "betas = results.getBetaValues()\n",
    "VTT   = 60*betas['B_TIME']/(betas['B_COST'])\n",
    "print(f'\\nThe Value-of-Travel-Time = {VTT:.3f} chf/hr.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ``Exercise 1: Interpreting the Value-of-Travel Time``\n",
    "`A` Interpret the estimation results of the MNL model in terms of the sing and sizes of the parameters<br>\n",
    "`B` The most recent Swiss value of travel time study finds a VTT of 27 ch/hr for car, and 14 chf/hr for Public Transport. <br>\n",
    "Could you think of reasons why the estimate found here is so much larger?<br>\n",
    "`C` Compute the Willingness to pay to go by SM instead of TRAIN, all esle being equal, using the formula: WtP_SM = -(ASC_SM-ASC_TRAIN) / B_COST<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WtP_SM = -betas['ASC_SM']/(betas['B_COST']/100)\n",
    "print(f'The willingness to pay to use SM instead of TRAIN (all else being equal) = \\t {WtP_SM:.2f} chf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. The hybrid ANN-MNL model**\n",
    "\n",
    "Next, we are going to train the hybrid ANN-MNL model. Since in this lab session we are interested in the Value-of-Time, we place the features of behavioural interest: travel cost and travel time, in the *MNL part of the model*. For the *ANN part of the model* we place the remaining features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Define lists of the features for the MNL and ANN parts**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this lab session we want to compute the VTT, using the hybrid model. There travel time and travel cost features go to the MNL part\n",
    "# All other possibly explanatory features go to the ANN part\n",
    "features_mnl = ['TRAIN_TT','TRAIN_CO','SM_TT','SM_CO','CAR_TT','CAR_CO']\n",
    "features_ann = ['PURPOSE', 'FIRST','TICKET', 'WHO', 'LUGGAGE', 'AGE','MALE', 'GA','INCOME', 'ORIGIN','DEST','TRAIN_HE','SM_HE','SM_SEATS','TRAIN_AV','SM_AV','CAR_AV']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Global variables and hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global variables (do not change)\n",
    "OBS = int(len(df))                      # Number of observations\n",
    "NALT = int(3)                           # Number of alterantives in the data set.\n",
    "no_X_MNL = int(len(features_mnl)/NALT)  # Number of attributes with behavioural interest (-->MNL model part).  In this example we are particularly interested in the VTT--> Cost & Tume\n",
    "no_X_ANN = int(len(features_ann))       # Number of features without behavioural interest (-->ANN model part). \n",
    "\n",
    "# Hyperparameters\n",
    "num_nodes = 10                          # Number of nodes in hidden layer(s). Again we use 2 hidden layers with *num_nodes* nodes each\n",
    "alpha = 0.00001                         # Governs the L2 regularisation\n",
    "nEpoch = 50                             # Max number epochs for training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the hybrid ANN-MNL object**\n",
    "We start with defining the network's layers and how they are connected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MNL part**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the MNL part, we use a convolution layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input layer for MNL part\n",
    "X_MNL = Input((no_X_MNL, NALT,1), name = 'Features2MNL')\n",
    "\n",
    "# Create the utility layer for the MNL part.\n",
    "# kernel_size = [no_X_MNL,1] defines the height and width of the convolution kernel\n",
    "# strides = (2,1) defines the stride\n",
    "# padding = 'valid' means that NO padding is used (not discussed in class)\n",
    "V_MNL = Conv2D(filters = 1, kernel_size = [no_X_MNL,1], strides = (2,1), padding = 'valid', name = 'MNL_layer', use_bias = False, trainable = True)(X_MNL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ANN part**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the ANN part, we use a dense layer. in tensorflow dense layers refer to fully connected MLP layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the input layer for the ANN part\n",
    "X_ANN = Input((no_X_ANN), name ='Features2ANN')\n",
    "\n",
    "# Create the hidden layers for the ANN part\n",
    "reg = keras.regularizers.L2(alpha)  # defines regularisation settings. Here we use L2 regularisation. \n",
    "layer1_ANN = Dense(units = num_nodes, name = \"ANN_HiddenLayer1\", use_bias = True, kernel_regularizer = reg)(X_ANN)      \n",
    "layer2_ANN = Dense(units = num_nodes, name = \"ANN_HiddenLayer2\", use_bias = True, kernel_regularizer = reg)(layer1_ANN)\n",
    "\n",
    "# Create the  \n",
    "V_ANN = Dense(units = NALT, name = \"V_ANN\", use_bias = True)(layer2_ANN) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part where the MNL and ANN parts come together**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the tensors to [1 X NALT]\n",
    "V_MNL = Reshape([NALT], name = 'Flatten_MNL')(V_MNL)\n",
    "V_ANN = Reshape([NALT], name = 'Flatten_ANN')(V_ANN) \n",
    "\n",
    "# Create a layer that simply sums the utilities of the MNL and ANN parts\n",
    "V_MNL_ANN = Add(name = \"sum_V_MNL_V_ANN\")([V_MNL,V_ANN])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Output layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use a logit output layer with V_MNL_ANN as input\n",
    "logitProb = Activation('softmax', name = 'Probability')(V_MNL_ANN)\n",
    "\n",
    "# Create the model, by putting everything together\n",
    "model = Model(inputs = [X_MNL, X_ANN], outputs = logitProb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Inspect the created model opject**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Does it have the expected shapes and number of weights (Param #)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`@ Francisco, could you add a picture of hybrid network here?`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Compile the model**\n",
    "When we are satisfied with the created model structure, we compile the model.<br>\n",
    "When we say we 'compile the model', we mean the model and the optimiser and wrapped together into a trainable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note that we use the Adam optimiser, with the shown settings\n",
    "opt = keras.optimizers.Adam(learning_rate=0.001,beta_1=0.9,beta_2=0.999,epsilon=1e-07)\n",
    "model.compile(loss='categorical_crossentropy', metrics = [\"accuracy\"], optimizer = opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Scale the features**\n",
    "Before training an ANN, we must scale the input features. However, since we want to compute VTTs the features of the MNL part cannot just be scaled by just any scaler. This would hamper the intepretation of the associated betas. Therefore, we must manually scale the features for the MNL part, while we can use the `sk-learn's` StandardScaler for the features for the ANN part. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling the features for MNL part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with features for MNL part\n",
    "x_mnl = df[features_mnl]\n",
    "\n",
    "# Manually scale all features by 100\n",
    "scale = 100\n",
    "x_mnl = np.divide(x_mnl,scale)\n",
    "\n",
    "# Convert into a numpy array and reshape so the convolution layer can handle it.\n",
    "# As convolution layers are designed for images, they take 4D tensors as inputs: Batch x Width x Height x RGB color channels\n",
    "# In this case we use only 1 color channel (as if the image is in greyscale)\n",
    "x_mnl = np.expand_dims(np.swapaxes(np.array(x_mnl.T).reshape((NALT,no_X_MNL,OBS)),axis1=2,axis2=0),3)\n",
    "print('Shape of x_mnl', x_mnl.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Scaling the features for ANN part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataframe with features for ANN part\n",
    "x_ann = df[features_ann]\n",
    "\n",
    "# Define the list of features that need to be scaled (excluding availabilities and the choice!)\n",
    "features2scale = ['PURPOSE', 'FIRST','TICKET', 'WHO', 'LUGGAGE', 'AGE','MALE', 'INCOME', 'GA', 'ORIGIN','DEST','TRAIN_HE','SM_HE','SM_SEATS']\n",
    "\n",
    "# Initiate scaler object & fit to features\n",
    "scaler = StandardScaler()  \n",
    "scaler.fit(x_ann.loc[:,features2scale])  \n",
    "\n",
    "# Create new dataframe containing both the scaled features AND the (unscaled) ones. In particular, we do not want to scale the availabilities\n",
    "x_ann = df[features_ann].copy()\n",
    "x_ann.at[:, features2scale] = scaler.transform(df.loc[:,features2scale])\n",
    "x_ann = np.array(x_ann)\n",
    "print('Shape of x_ann',x_ann.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Creating the training and test data sets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with the output target\n",
    "Y = df['CHOICE']\n",
    "\n",
    "# Unlike sk-learn, Tensorflow requires the targets to be dummmy coded\n",
    "Y_cat = to_categorical(Y-1, num_classes = NALT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into a training and test part\n",
    "# We split both the features for the MNL and ANN part\n",
    "# We use the same random state. Thereby we make sure that the train and test for the MNL and ANN part contain the same observations\n",
    "X_mnl_train, X_mnl_test, Y_train, Y_test = train_test_split(x_mnl, Y_cat, random_state = 1, test_size = 0.35)\n",
    "X_ann_train, X_ann_test, Y_train, Y_test = train_test_split(x_ann, Y_cat, random_state = 1, test_size = 0.35)\n",
    "print('Total number of observations in the data set = ', len(x_mnl))\n",
    "print('Number of observations in the training set   = ', len(X_mnl_train))\n",
    "print('Number of observations in the test set       = ', len(X_mnl_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training the model**\n",
    "Finally, we are ready to train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = model.fit([X_mnl_train, X_ann_train],Y_train, epochs = nEpoch, verbose = 1, validation_data = ([X_mnl_test, X_ann_test], Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's have a look at the training progress, in terms of loss and accuracy\n",
    "# plot loss as a function of epochs\n",
    "fig, axes =plt.subplots(1, 1, figsize=(16, 6))\n",
    "plt.subplot(121)\n",
    "plt.title('Cross Entropy Loss')\n",
    "plt.plot(history.history['loss'],     color = 'blue', label = 'train')\n",
    "plt.plot(history.history['val_loss'], color = 'red', label = 'test')\n",
    "plt.ylim(0.6,1)\n",
    "plt.legend()\n",
    "\n",
    "# plot accuracy\n",
    "plt.subplot(122)\n",
    "plt.title('Classification Accuracy')\n",
    "plt.plot(history.history['accuracy'],     color = 'blue', label = 'train')\n",
    "plt.plot(history.history['val_accuracy'], color = 'red', label = 'test')\n",
    "plt.ylim(0, 1)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Exercise 2:`\n",
    "`A` Training time of the hybrid ANN-MNL model using tensorflow is considerably slower than of the MLP (trained in sk-learn). Can you think of reasons why?<br>\n",
    "`B` Performing a full-fletch hypertuning would be recommended, but is too time-comsuming for this lab session. Manually test the sensitivity to L2 regularisation (which turned out to be particularly important for the MLP). To do so, re-run this notebook with alpha is {1,0.1,0.00001}. Report you results. What L2 works best?<br><br>\n",
    "`**After you are finished testing, set the L2 to the best value found.**`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **4. Evaluating and comparing performances across trained models**<br>\n",
    "Machine learning researchers usually evaluate the performance of a classification model using the cross-entropy. while choice modellers usually look at the Log-likelihood (LL) and the rho-square.<br>\n",
    "As there is no standard function that outputs the evaluation metrics of both disciplines, below we create our **own evaluation function**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create an eavluation function that returns key evaluation metrics: LL, LL0, cross_entropy, rho_sq\n",
    "# To compute these performance metrics, the function takes as inputs:\n",
    "#   - the predicted probabilities (prob)\n",
    "#   - the choices (Y)\n",
    "#   - the availabilities (AV)\n",
    "def eval_performance(prob,Y,AV):\n",
    "    \n",
    "    # Calculate the likelihood of the data given the model\n",
    "    LL = np.sum(np.log(np.sum(prob*Y,axis=1)))\n",
    "\n",
    "    # Calculate the Null-loglikelihood\n",
    "    LL0 = np.sum(np.log(np.divide(1,np.sum(AV,axis=1))))\n",
    "\n",
    "    # Calculate cross-entropy\n",
    "    cross_entropy =  -LL/len(AV)\n",
    "    \n",
    "    # Calculate the rho_sq\n",
    "    rho_sq = 1 -(LL/LL0)\n",
    "\n",
    "    return LL, LL0, cross_entropy, rho_sq\n",
    "\n",
    "# Evaluate performance on the training and test data sets of the hybird ANN-MNL model\n",
    "eval_train = eval_performance(model.predict([X_mnl_train, X_ann_train]),Y_train, X_ann_train[:,[-3,-2,-1]])\n",
    "eval_test  = eval_performance(model.predict([X_mnl_test , X_ann_test ]),Y_test , X_ann_test[:,[-3,-2,-1]])\n",
    "\n",
    "# Print the results\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "results = pd.DataFrame({'data set':     ['Train','Test'],\n",
    "                        'LL':           [eval_train[0], eval_test[0]],\n",
    "                        'LL0':          [eval_train[1], eval_test[1]],\n",
    "                        'cross_entropy':[eval_train[2], eval_test[2]],\n",
    "                        'rho_sq':       [eval_train[3], eval_test[3]]})\n",
    "\n",
    "print('Model performance of the hybrid ANN-MNL model')\n",
    "print(results.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For comparison, we also evaluate performance on the training and test data sets of the linear-additive RUM model\n",
    "eval_mnl = eval_performance(prob_mnl,np.transpose([Y ==1,Y ==2,Y ==3]), df[['TRAIN_AV','SM_AV','CAR_AV']])\n",
    "\n",
    "# Print the results\n",
    "pd.options.display.float_format = '{:,.2f}'.format\n",
    "results = pd.DataFrame({'data set':     ['All data'],\n",
    "                        'LL':           [eval_mnl[0]],\n",
    "                        'LL0':          [eval_mnl[1]],\n",
    "                        'cross_entropy':[eval_mnl[2]],\n",
    "                        'rho_sq':       [eval_mnl[3]]})\n",
    "\n",
    "print('Model performance of the RUM-MNL model')\n",
    "print(results.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the weights of the MNL_layer\n",
    "betas_layer = model.get_layer(name = 'MNL_layer')\n",
    "betas = betas_layer.get_weights()\n",
    "beta_TT = np.squeeze((betas[0][0]))\n",
    "beta_TC = np.squeeze((betas[0][1]))\n",
    "print(f'Beta_TT = \\t{beta_TT:.3f}') \n",
    "print(f'Beta_TC = \\t{beta_TC:.3f})')\n",
    "\n",
    "# Compute VTT\n",
    "VTT = 60*(beta_TT/beta_TC)\n",
    "print(f'The Value-of-Time is {VTT:.2f} Swiss franc per hour')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the choices (hits) using the MNL and hybrid-ANN-MNL models\n",
    "Y_pred_mnl = np.argmax(prob_mnl, axis =1)+1 # <-- Argument for finding the column with the highest predicted probability for the MNL model\n",
    "Y_pred_hybrid = np.expand_dims(np.argmax(model.predict([x_mnl, x_ann]),axis = 1),axis =1)+1\n",
    "\n",
    "# Show the confusion matrices to asses difference between the predictions of the MNL and hybrid ANN-MNL models\n",
    "ylabels = ['TRAIN','SM','CAR']\n",
    "fig, ax = plt.subplots(2,2,figsize = (14,10))\n",
    "fig.set_tight_layout(True)\n",
    "\n",
    "# Dsiplay confusion matrices, non-normalised and normalised\n",
    "cm1 = ConfusionMatrixDisplay.from_predictions(y_true=Y,y_pred=Y_pred_mnl,    display_labels = ylabels, normalize=None,  ax=ax[(0,0)])\n",
    "cm2 = ConfusionMatrixDisplay.from_predictions(y_true=Y,y_pred=Y_pred_mnl,    display_labels = ylabels, normalize='true',ax=ax[(1,0)])\n",
    "cm3 = ConfusionMatrixDisplay.from_predictions(y_true=Y,y_pred=Y_pred_hybrid, display_labels = ylabels, normalize=None,  ax=ax[(0,1)])\n",
    "cm4 = ConfusionMatrixDisplay.from_predictions(y_true=Y,y_pred=Y_pred_hybrid, display_labels = ylabels, normalize='true',ax=ax[(1,1)])\n",
    "\n",
    "# Add titles to confusion matrices\n",
    "cm1.ax_.set_title('RUM_MNL predictions')\n",
    "cm2.ax_.set_title('RUM_MNL predictions - normalised')\n",
    "cm3.ax_.set_title('Hybrid ANN-MNL')\n",
    "cm4.ax_.set_title('Hybrid ANN-MNL - normalised')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Exercise 3: Model performance evaluation and comparison`<br>\n",
    "`A` Interpret and explain the difference in the model cross entropy performance of the RUM-MNL, Hybrid ANN-MNL, and ANN (previous lab session)<br>\n",
    "`B` Show the Precision, Recall, and F1 score for the hybrid ANN-MNL and for the RUM-MNL, using `sk-learn's` `classification_report` function and interpret the results<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To calculate the precision, recal and f1 score we conveniently use sk-learn's 'classification_report' functionality\n",
    "print('Classification report forthe RUM-MNL model\\n',\n",
    "    classification_report(Y,Y_pred_mnl, target_names= ylabels))\n",
    "print('\\nClassification report for hybrid ANN-MNL model\\n',\n",
    "    classification_report(Y,Y_pred_hybrid, target_names= ylabels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### `Exercise 4: Hand-engineering for interactions with cost needed?`\n",
    "In part 1 ('Preparing your data') we set the cost for TRAIN and SM to zero for concession card holders (GA == 1). <br>\n",
    "`A` Re-run your whole notebook, but without setting the cost for TRAIN and SM to zero. <br>\n",
    "Look at the model performance (e.g. cross entropy) as well as the VTT. What catches the eye?<br>\n",
    "`B` Could you think of a reason that explains the difference between the run with and without having set the cost to zero for TRAIN and SM?<br>"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
